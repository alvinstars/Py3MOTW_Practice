{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# json - JavaScript Object Notation\n",
    "\n",
    "Purpose:\tEncode Python objects as JSON strings, and decode JSON strings into Python objects.\n",
    "    \n",
    "The json module provides an API similar to pickle for converting in-memory Python objects to a serialized representation known as JavaScript Object Notation (JSON). Unlike pickle, JSON has the benefit of having implementations in many languages (especially JavaScript). It is most widely used for communicating between the web server and client in a REST API, but is also useful for other inter-application communication needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding and Decoding Simple Data Types\n",
    "\n",
    "The encoder understands Python’s native types by default (str, int, float, list, tuple, and dict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA: [{'c': 3.0, 'a': 'A', 'b': (2, 4)}]\n",
      "JSON: [{\"c\": 3.0, \"a\": \"A\", \"b\": [2, 4]}]\n"
     ]
    }
   ],
   "source": [
    "# json_simple_types.py\n",
    "import json\n",
    "\n",
    "data = [{'a': 'A', 'b': (2, 4), 'c': 3.0}]\n",
    "print('DATA:', repr(data))\n",
    "\n",
    "data_string = json.dumps(data)\n",
    "print('JSON:', data_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values are encoded in a manner superficially similar to Python’s repr() output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding, then re-decoding may not give exactly the same type of object.\n",
    "\n",
    "In particular, tuples become lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA   : [{'c': 3.0, 'a': 'A', 'b': (2, 4)}]\n",
      "ENCODED: [{\"c\": 3.0, \"a\": \"A\", \"b\": [2, 4]}]\n",
      "DECODED: [{'c': 3.0, 'a': 'A', 'b': [2, 4]}]\n",
      "ORIGINAL: <class 'tuple'>\n",
      "DECODED : <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# json_simple_types_decode.py\n",
    "import json\n",
    "\n",
    "data = [{'a': 'A', 'b': (2, 4), 'c': 3.0}]\n",
    "print('DATA   :', data)\n",
    "\n",
    "data_string = json.dumps(data)\n",
    "print('ENCODED:', data_string)\n",
    "\n",
    "decoded = json.loads(data_string)\n",
    "print('DECODED:', decoded)\n",
    "\n",
    "print('ORIGINAL:', type(data[0]['b']))\n",
    "print('DECODED :', type(decoded[0]['b']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human-consumable vs. Compact Output\n",
    "\n",
    "Another benefit of JSON over pickle is that the results are human-readable. The dumps() function accepts several arguments to make the output even nicer. For example, the sort_keys flag tells the encoder to output the keys of a dictionary in sorted, instead of random, order.\n",
    "\n",
    "Sorting makes it easier to scan the results by eye, and also makes it possible to compare JSON output in tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA: [{'c': 3.0, 'a': 'A', 'b': (2, 4)}]\n",
      "JSON: [{\"c\": 3.0, \"a\": \"A\", \"b\": [2, 4]}]\n",
      "SORT: [{\"a\": \"A\", \"b\": [2, 4], \"c\": 3.0}]\n",
      "UNSORTED MATCH: False\n",
      "SORTED MATCH  : True\n"
     ]
    }
   ],
   "source": [
    "# json_sort_keys.py\n",
    "import json\n",
    "\n",
    "data = [{'a': 'A', 'b': (2, 4), 'c': 3.0}]\n",
    "print('DATA:', repr(data))\n",
    "\n",
    "unsorted = json.dumps(data)\n",
    "print('JSON:', json.dumps(data))\n",
    "print('SORT:', json.dumps(data, sort_keys=True))\n",
    "\n",
    "first = json.dumps(data, sort_keys=True)\n",
    "second = json.dumps(data, sort_keys=True)\n",
    "\n",
    "print('UNSORTED MATCH:', unsorted == first)\n",
    "print('SORTED MATCH  :', first == second)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For highly-nested data structures, specify a value for indent so the output is formatted nicely as well.\n",
    "\n",
    "When indent is a non-negative integer, the output more closely resembles that of pprint, with leading spaces for each level of the data structure matching the indent level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA: [{'c': 3.0, 'a': 'A', 'b': (2, 4)}]\n",
      "NORMAL: [{\"a\": \"A\", \"b\": [2, 4], \"c\": 3.0}]\n",
      "INDENT: [\n",
      "  {\n",
      "    \"a\": \"A\",\n",
      "    \"b\": [\n",
      "      2,\n",
      "      4\n",
      "    ],\n",
      "    \"c\": 3.0\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# json_indent.py\n",
    "import json\n",
    "\n",
    "data = [{'a': 'A', 'b': (2, 4), 'c': 3.0}]\n",
    "print('DATA:', repr(data))\n",
    "\n",
    "print('NORMAL:', json.dumps(data, sort_keys=True))\n",
    "print('INDENT:', json.dumps(data, sort_keys=True, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verbose output like this increases the number of bytes needed to transmit the same amount of data, however, so it is not intended for use in a production environment. In fact, it is possible to adjust the settings for separating data in the encoded output to make it even more compact than the default.\n",
    "\n",
    "The separators argument to dumps() should be a tuple containing the strings to separate items in a list and keys from values in a dictionary. The default is (', ', ': '). By removing the whitespace, a more compact output is produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA: [{'c': 3.0, 'a': 'A', 'b': (2, 4)}]\n",
      "repr(data)             : 35\n",
      "dumps(data)            : 35\n",
      "dumps(data, indent=2)  : 73\n",
      "dumps(data, separators): 29\n"
     ]
    }
   ],
   "source": [
    "# json_compact_encoding.py\n",
    "import json\n",
    "\n",
    "data = [{'a': 'A', 'b': (2, 4), 'c': 3.0}]\n",
    "print('DATA:', repr(data))\n",
    "\n",
    "print('repr(data)             :', len(repr(data)))\n",
    "\n",
    "plain_dump = json.dumps(data)\n",
    "print('dumps(data)            :', len(plain_dump))\n",
    "\n",
    "small_indent = json.dumps(data, indent=2)\n",
    "print('dumps(data, indent=2)  :', len(small_indent))\n",
    "\n",
    "with_separators = json.dumps(data, separators=(',', ':'))\n",
    "print('dumps(data, separators):', len(with_separators))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Dictionaries\n",
    "\n",
    "The JSON format expects the keys to a dictionary to be strings. Trying to encode a dictionary with non-string types as keys produces a TypeError. One way to work around that limitation is to tell the encoder to skip over non-string keys using the skipkeys argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First attempt\n",
      "ERROR: keys must be a string\n",
      "\n",
      "Second attempt\n",
      "[{\"c\": 3.0, \"a\": \"A\", \"b\": [2, 4]}]\n"
     ]
    }
   ],
   "source": [
    "# json_skipkeys.py\n",
    "import json\n",
    "\n",
    "data = [{'a': 'A', 'b': (2, 4), 'c': 3.0, ('d',): 'D tuple'}]\n",
    "\n",
    "print('First attempt')\n",
    "try:\n",
    "    print(json.dumps(data))\n",
    "except TypeError as err:\n",
    "    print('ERROR:', err)\n",
    "\n",
    "print()\n",
    "print('Second attempt')\n",
    "print(json.dumps(data, skipkeys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than raising an exception, the non-string key is ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Custom Types\n",
    "\n",
    "All of the examples so far have used Pythons built-in types because those are supported by json natively. It is common to need to encode custom classes, as well, and there are two ways to do that.\n",
    "\n",
    "Given this class to encode:\n",
    "\n",
    "json_myobj.py\n",
    "\n",
    "class MyObj:\n",
    "\n",
    "    def __init__(self, s):\n",
    "        self.s = s\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '<MyObj({})>'.format(self.s)\n",
    "\n",
    "The simple way of encoding a MyObj instance is to define a function to convert an unknown type to a known type. It does not need to do the encoding, so it should just convert one object to another.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First attempt\n",
      "ERROR: <MyObj(instance value goes here)> is not JSON serializable\n",
      "\n",
      "With default\n",
      "default( <MyObj(instance value goes here)> )\n",
      "{\"__module__\": \"json_myobj\", \"s\": \"instance value goes here\", \"__class__\": \"MyObj\"}\n"
     ]
    }
   ],
   "source": [
    "# son_dump_default.py\n",
    "import json\n",
    "import json_myobj\n",
    "\n",
    "obj = json_myobj.MyObj('instance value goes here')\n",
    "\n",
    "print('First attempt')\n",
    "try:\n",
    "    print(json.dumps(obj))\n",
    "except TypeError as err:\n",
    "    print('ERROR:', err)\n",
    "\n",
    "\n",
    "def convert_to_builtin_type(obj):\n",
    "    print('default(', repr(obj), ')')\n",
    "    # Convert objects to a dictionary of their representation\n",
    "    d = {\n",
    "        '__class__': obj.__class__.__name__,\n",
    "        '__module__': obj.__module__,\n",
    "    }\n",
    "    d.update(obj.__dict__)\n",
    "    return d\n",
    "\n",
    "\n",
    "print()\n",
    "print('With default')\n",
    "print(json.dumps(obj, default=convert_to_builtin_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In convert_to_builtin_type(), instances of classes not recognized by json are converted to dictionaries with enough information to re-create the object if a program has access to the Python modules necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To decode the results and create a MyObj() instance, use the object_hook argument to loads() to tie in to the decoder so the class can be imported from the module and used to create the instance.\n",
    "\n",
    "The object_hook is called for each dictionary decoded from the incoming data stream, providing a chance to convert the dictionary to another type of object. The hook function should return the object the calling application should receive instead of the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODULE: json_myobj\n",
      "CLASS: <class 'json_myobj.MyObj'>\n",
      "INSTANCE ARGS: {'s': 'instance value goes here'}\n",
      "[<MyObj(instance value goes here)>]\n"
     ]
    }
   ],
   "source": [
    "# json_load_object_hook.py\n",
    "import json\n",
    "\n",
    "def dict_to_object(d):\n",
    "    if '__class__' in d:\n",
    "        class_name = d.pop('__class__')\n",
    "        module_name = d.pop('__module__')\n",
    "        module = __import__(module_name)\n",
    "        print('MODULE:', module.__name__)\n",
    "        class_ = getattr(module, class_name)\n",
    "        print('CLASS:', class_)\n",
    "        args = {\n",
    "            key: value\n",
    "            for key, value in d.items()\n",
    "        }\n",
    "        print('INSTANCE ARGS:', args)\n",
    "        inst = class_(**args)\n",
    "    else:\n",
    "        inst = d\n",
    "    return inst\n",
    "\n",
    "\n",
    "encoded_object = '''\n",
    "    [{\"s\": \"instance value goes here\",\n",
    "      \"__module__\": \"json_myobj\", \"__class__\": \"MyObj\"}]\n",
    "    '''\n",
    "\n",
    "myobj_instance = json.loads(\n",
    "    encoded_object,\n",
    "    object_hook=dict_to_object,\n",
    ")\n",
    "print(myobj_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since json converts string values to unicode objects, they need to be re-encoded as ASCII strings before they can be used as keyword arguments to the class constructor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar hooks are available for the built-in types integers (parse_int), floating point numbers (parse_float), and constants (parse_constant)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder and Decoder Classes\n",
    "\n",
    "Besides the convenience functions already covered, the json module provides classes for encoding and decoding. Using the classes directly gives access to extra APIs for customizing their behavior.\n",
    "\n",
    "The JSONEncoder uses an iterable interface for producing “chunks” of encoded data, making it easier to write to files or network sockets without having to represent an entire data structure in memory.\n",
    "\n",
    "The output is generated in logical units, rather than being based on any size value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART: [\n",
      "PART: {\n",
      "PART: \"c\"\n",
      "PART: : \n",
      "PART: 3.0\n",
      "PART: , \n",
      "PART: \"a\"\n",
      "PART: : \n",
      "PART: \"A\"\n",
      "PART: , \n",
      "PART: \"b\"\n",
      "PART: : \n",
      "PART: [2\n",
      "PART: , 4\n",
      "PART: ]\n",
      "PART: }\n",
      "PART: ]\n"
     ]
    }
   ],
   "source": [
    "# json_encoder_iterable.py\n",
    "import json\n",
    "\n",
    "encoder = json.JSONEncoder()\n",
    "data = [{'a': 'A', 'b': (2, 4), 'c': 3.0}]\n",
    "\n",
    "for part in encoder.iterencode(data):\n",
    "    print('PART:', part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encode() method is basically equivalent to ''.join(encoder.iterencode()), with some extra error checking up front.\n",
    "\n",
    "To encode arbitrary objects, override the default() method with an implementation similar to the one used in convert_to_builtin_type().\n",
    "\n",
    "The output is the same as the previous implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MyObj(internal data)>\n",
      "default( <MyObj(internal data)> )\n",
      "{\"__module__\": \"json_myobj\", \"s\": \"internal data\", \"__class__\": \"MyObj\"}\n"
     ]
    }
   ],
   "source": [
    "# json_encoder_default.py\n",
    "import json\n",
    "import json_myobj\n",
    "\n",
    "\n",
    "class MyEncoder(json.JSONEncoder):\n",
    "\n",
    "    def default(self, obj):\n",
    "        print('default(', repr(obj), ')')\n",
    "        # Convert objects to a dictionary of their representation\n",
    "        d = {\n",
    "            '__class__': obj.__class__.__name__,\n",
    "            '__module__': obj.__module__,\n",
    "        }\n",
    "        d.update(obj.__dict__)\n",
    "        return d\n",
    "\n",
    "\n",
    "obj = json_myobj.MyObj('internal data')\n",
    "print(obj)\n",
    "print(MyEncoder().encode(obj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Streams and Files\n",
    "\n",
    "All of the examples so far have assumed that the encoded version of the entire data structure could be held in memory at one time. With large data structures, it may be preferable to write the encoding directly to a file-like object. The convenience functions load() and dump() accept references to a file-like object to use for reading or writing.\n",
    "\n",
    "A socket or normal file handle would work the same way as the StringIO buffer used in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"c\": 3.0, \"a\": \"A\", \"b\": [2, 4]}]\n"
     ]
    }
   ],
   "source": [
    "# json_dump_file.py\n",
    "import io\n",
    "import json\n",
    "\n",
    "data = [{'a': 'A', 'b': (2, 4), 'c': 3.0}]\n",
    "\n",
    "f = io.StringIO()\n",
    "json.dump(data, f)\n",
    "\n",
    "print(f.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it is not optimized to read only part of the data at a time, the load() function still offers the benefit of encapsulating the logic of generating objects from stream input.\n",
    "\n",
    "Just as for dump(), any file-like object can be passed to load()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'c': 3.0, 'a': 'A', 'b': [2, 4]}]\n"
     ]
    }
   ],
   "source": [
    "# json_load_file.py\n",
    "import io\n",
    "import json\n",
    "\n",
    "f = io.StringIO('[{\"a\": \"A\", \"c\": 3.0, \"b\": [2, 4]}]')\n",
    "print(json.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed Data Streams\n",
    "\n",
    "JSONDecoder includes raw_decode(), a method for decoding a data structure followed by more data, such as JSON data with trailing text. The return value is the object created by decoding the input data, and an index into that data indicating where decoding left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON first:\n",
      "Object              : [{'c': 3.0, 'a': 'A', 'b': [2, 4]}]\n",
      "End of parsed input : 35\n",
      "Remaining text      : ' This text is not JSON.'\n",
      "\n",
      "JSON embedded:\n",
      "ERROR: Expecting value: line 1 column 1 (char 0)\n"
     ]
    }
   ],
   "source": [
    "# json_mixed_data.py\n",
    "import json\n",
    "\n",
    "decoder = json.JSONDecoder()\n",
    "\n",
    "def get_decoded_and_remainder(input_data):\n",
    "    obj, end = decoder.raw_decode(input_data)\n",
    "    remaining = input_data[end:]\n",
    "    return (obj, end, remaining)\n",
    "\n",
    "\n",
    "encoded_object = '[{\"a\": \"A\", \"c\": 3.0, \"b\": [2, 4]}]'\n",
    "extra_text = 'This text is not JSON.'\n",
    "\n",
    "print('JSON first:')\n",
    "data = ' '.join([encoded_object, extra_text])\n",
    "obj, end, remaining = get_decoded_and_remainder(data)\n",
    "\n",
    "print('Object              :', obj)\n",
    "print('End of parsed input :', end)\n",
    "print('Remaining text      :', repr(remaining))\n",
    "\n",
    "print()\n",
    "print('JSON embedded:')\n",
    "try:\n",
    "    data = ' '.join([extra_text, encoded_object, extra_text])\n",
    "    obj, end, remaining = get_decoded_and_remainder(data)\n",
    "except ValueError as err:\n",
    "    print('ERROR:', err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON at the Command Line\n",
    "\n",
    "The json.tool module implements a command line program for reformatting JSON data to be easier to read.\n",
    "\n",
    "[{\"a\": \"A\", \"c\": 3.0, \"b\": [2, 4]}]\n",
    "\n",
    "The input file example.json contains a mapping with the keys out of alphabetical order. The first example below shows the data reformatted in order, and the second example uses --sort-keys to sort the mapping keys before printing the output."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ python3 -m json.tool example.json"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ python3 -m json.tool --sort-keys example.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
