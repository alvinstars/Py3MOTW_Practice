{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# urllib.robotparser - Internet Spider Access Control\n",
    "\n",
    "Purpose:\tParse robots.txt file used to control Internet spiders\n",
    "\n",
    "robotparser implements a parser for the robots.txt file format, including a function that checks if a given user agent can access a resource. It is intended for use in well-behaved spiders, or other crawler applications that need to either be throttled or otherwise restricted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## robots.txt\n",
    "\n",
    "The robots.txt file format is a simple text-based access control system for computer programs that automatically access web resources (“spiders”, “crawlers”, etc.). The file is made up of records that specify the user agent identifier for the program followed by a list of URLs (or URL prefixes) the agent may not access.\n",
    "\n",
    "This is a sample robots.txt file "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# robots.txt\n",
    "\n",
    "Sitemap: https://pymotw.com/sitemap.xml\n",
    "User-agent: *\n",
    "Disallow: /admin/\n",
    "Disallow: /downloads/\n",
    "Disallow: /media/\n",
    "Disallow: /static/\n",
    "Disallow: /codehosting/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It prevents access to some of the parts of the site that are expensive to compute and would overload the server if a search engine tried to index them. For a more complete set of examples of robots.txt, refer to The Web Robots Page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Access Permissions\n",
    "\n",
    "Using the data presented earlier, a simple crawler can test whether it is allowed to download a page using RobotFileParser.can_fetch()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  True : /\n",
      "  True : https://pymotw.com/\n",
      "\n",
      "  True : /PyMOTW/\n",
      "  True : https://pymotw.com/PyMOTW/\n",
      "\n",
      " False : /admin/\n",
      " False : https://pymotw.com/admin/\n",
      "\n",
      " False : /downloads/PyMOTW-1.92.tar.gz\n",
      " False : https://pymotw.com/downloads/PyMOTW-1.92.tar.gz\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# urllib_robotparser_simple.py\n",
    "from urllib import parse\n",
    "from urllib import robotparser\n",
    "\n",
    "AGENT_NAME = 'PyMOTW'\n",
    "URL_BASE = 'https://pymotw.com/'\n",
    "parser = robotparser.RobotFileParser()\n",
    "parser.set_url(parse.urljoin(URL_BASE, 'robots.txt'))\n",
    "parser.read()\n",
    "\n",
    "PATHS = [\n",
    "    '/',\n",
    "    '/PyMOTW/',\n",
    "    '/admin/',\n",
    "    '/downloads/PyMOTW-1.92.tar.gz',\n",
    "]\n",
    "\n",
    "for path in PATHS:\n",
    "    print('{!r:>6} : {}'.format(\n",
    "        parser.can_fetch(AGENT_NAME, path), path))\n",
    "    url = parse.urljoin(URL_BASE, path)\n",
    "    print('{!r:>6} : {}'.format(\n",
    "        parser.can_fetch(AGENT_NAME, url), url))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The URL argument to can_fetch() can be a path relative to the root of the site, or full URL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long-lived Spiders\n",
    "\n",
    "An application that takes a long time to process the resources it downloads or that is throttled to pause between downloads should check for new robots.txt files periodically based on the age of the content it has downloaded already. The age is not managed automatically, but there are convenience methods to make tracking it easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age: 0 \n",
      "  True : /\n",
      "\n",
      "age: 2 rereading robots.txt\n",
      "  True : /PyMOTW/\n",
      "\n",
      "age: 2 rereading robots.txt\n",
      " False : /admin/\n",
      "\n",
      "age: 2 rereading robots.txt\n",
      " False : /downloads/PyMOTW-1.92.tar.gz\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# urllib_robotparser_longlived.py\n",
    "from urllib import robotparser\n",
    "import time\n",
    "\n",
    "AGENT_NAME = 'PyMOTW'\n",
    "parser = robotparser.RobotFileParser()\n",
    "# Using the local copy\n",
    "parser.set_url('file:robots.txt')\n",
    "parser.read()\n",
    "parser.modified()\n",
    "\n",
    "PATHS = [\n",
    "    '/',\n",
    "    '/PyMOTW/',\n",
    "    '/admin/',\n",
    "    '/downloads/PyMOTW-1.92.tar.gz',\n",
    "]\n",
    "\n",
    "for path in PATHS:\n",
    "    age = int(time.time() - parser.mtime())\n",
    "    print('age:', age, end=' ')\n",
    "    if age > 1:\n",
    "        print('rereading robots.txt')\n",
    "        parser.read()\n",
    "        parser.modified()\n",
    "    else:\n",
    "        print()\n",
    "    print('{!r:>6} : {}'.format(\n",
    "        parser.can_fetch(AGENT_NAME, path), path))\n",
    "    # Simulate a delay in processing\n",
    "    time.sleep(2)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This extreme example downloads a new robots.txt file if the one it has is more than one second old.\n",
    "\n",
    "A nicer version of the long-lived application might request the modification time for the file before downloading the entire thing. On the other hand, robots.txt files are usually fairly small, so it is not that much more expensive to just retrieve the entire document again."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
