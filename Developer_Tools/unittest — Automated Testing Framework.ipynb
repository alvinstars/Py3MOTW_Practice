{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unittest — Automated Testing Framework\n",
    "\n",
    "Purpose:\tAutomated testing framework\n",
    "\n",
    "Python’s unittest module is based on the XUnit framework design by Kent Beck and Erich Gamma. The same pattern is repeated in many other languages, including C, Perl, Java, and Smalltalk. The framework implemented by unittest supports fixtures, test suites, and a test runner to enable automated testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Test Structure\n",
    "\n",
    "Tests, as defined by unittest, have two parts: code to manage test dependencies (called fixtures), and the test itself. Individual tests are created by subclassing TestCase and overriding or adding appropriate methods. In the following example, the SimplisticTest has a single test() method, which would fail if a is ever different from b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# unittest_simple.py\n",
    "import unittest \n",
    "\n",
    "class SimplisticTest(unittest.TestCase):\n",
    "    def test(self):\n",
    "        a = 'a'\n",
    "        b = 'a'\n",
    "        self.assertEqual(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Tests\n",
    "\n",
    "The easiest way to run unittest tests is use the automatic discovery available through the command line interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.000s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python3 -m unittest unittest_simple.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This abbreviated output includes the amount of time the tests took, along with a status indicator for each test (the ”.” on the first line of output means that a test passed). For more detailed test results, include the -v option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test (unittest_simple.SimplisticTest) ... ok\r\n",
      "\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 1 test in 0.001s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!python3 -m unittest -v unittest_simple.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Outcomes\n",
    "\n",
    "Tests have 3 possible outcomes, described in the table below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Outcome | Description                                                     |\n",
    "|---------|-----------------------------------------------------------------|\n",
    "| ok      | The test passes.                                                |\n",
    "| FAIL    | The test does not pass, and raises an AssertionError exception. |\n",
    "| ERROR   | The test raises any exception other than AssertionError.        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no explicit way to cause a test to “pass”, so a test’s status depends on the presence (or absence) of an exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# unittest_outcomes.py\n",
    "\n",
    "import unittest\n",
    "\n",
    "class OutcomesTest(unittest.TestCase):\n",
    "    def testPass(self):\n",
    "        pass\n",
    "    def testFail(self):\n",
    "        self.assertFalse(True)\n",
    "    def testError(self):\n",
    "        raise RuntimeError('Test error!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a test fails or generates an error, the traceback is included in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EF.\r\n",
      "======================================================================\r\n",
      "ERROR: testError (unittest_outcomes.OutcomesTest)\r\n",
      "----------------------------------------------------------------------\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/Users/binyang/GitHub/Py3MOTW/Developer_Tools/unittest_outcomes.py\", line 11, in testError\r\n",
      "    raise RuntimeError('Test error!')\r\n",
      "RuntimeError: Test error!\r\n",
      "\r\n",
      "======================================================================\r\n",
      "FAIL: testFail (unittest_outcomes.OutcomesTest)\r\n",
      "----------------------------------------------------------------------\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/Users/binyang/GitHub/Py3MOTW/Developer_Tools/unittest_outcomes.py\", line 9, in testFail\r\n",
      "    self.assertFalse(True)\r\n",
      "AssertionError: True is not false\r\n",
      "\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 3 tests in 0.003s\r\n",
      "\r\n",
      "FAILED (failures=1, errors=1)\r\n"
     ]
    }
   ],
   "source": [
    "! python3 -m unittest unittest_outcomes.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example, testFail() fails and the traceback shows the line with the failure code. It is up to the person reading the test output to look at the code to figure out the meaning of the failed test, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# unittest_failwithmessage.py\n",
    "import unittest\n",
    "\n",
    "\n",
    "class FailureMessageTest(unittest.TestCase):\n",
    "\n",
    "    def testFail(self):\n",
    "        self.assertFalse(True, 'failure message goes here')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it easier to understand the nature of a test failure, the fail*() and assert*() methods all accept an argument msg, which can be used to produce a more detailed error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testFail (unittest_failwithmessage.FailureMessageTest) ... FAIL\r\n",
      "\r\n",
      "======================================================================\r\n",
      "FAIL: testFail (unittest_failwithmessage.FailureMessageTest)\r\n",
      "----------------------------------------------------------------------\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/Users/binyang/GitHub/Py3MOTW/Developer_Tools/unittest_failwithmessage.py\", line 8, in testFail\r\n",
      "    self.assertFalse(True, 'failure message goes here')\r\n",
      "AssertionError: True is not false : failure message goes here\r\n",
      "\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 1 test in 0.001s\r\n",
      "\r\n",
      "FAILED (failures=1)\r\n"
     ]
    }
   ],
   "source": [
    "! python3 -m unittest -v unittest_failwithmessage.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asserting Truth\n",
    "\n",
    "Most tests assert the truth of some condition. There are two different ways to write truth-checking tests, depending on the perspective of the test author and the desired outcome of the code being tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# unittest_truth.py\n",
    "import unittest\n",
    "\n",
    "class TruthTest(unittest.TestCase):\n",
    "\n",
    "    def testAssertTrue(self):\n",
    "        self.assertTrue(True)\n",
    "\n",
    "    def testAssertFalse(self):\n",
    "        self.assertFalse(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the code produces a value which can be evaluated as true, the method assertTrue() should be used. If the code produces a false value, the method assertFalse() make more sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testAssertFalse (unittest_truth.TruthTest) ... ok\r\n",
      "testAssertTrue (unittest_truth.TruthTest) ... ok\r\n",
      "\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 2 tests in 0.000s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "! python3 -m unittest -v unittest_truth.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Equality\n",
    "\n",
    "As a special case, unittest includes methods for testing the equality of two values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#unittest_equality.py\n",
    "import unittest\n",
    "\n",
    "\n",
    "class EqualityTest(unittest.TestCase):\n",
    "\n",
    "    def testExpectEqual(self):\n",
    "        self.assertEqual(1, 3 - 2)\n",
    "\n",
    "    def testExpectEqualFails(self):\n",
    "        self.assertEqual(2, 3 - 2)\n",
    "\n",
    "    def testExpectNotEqual(self):\n",
    "        self.assertNotEqual(2, 3 - 2)\n",
    "\n",
    "    def testExpectNotEqualFails(self):\n",
    "        self.assertNotEqual(1, 3 - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testExpectEqual (unittest_equality.EqualityTest) ... ok\r\n",
      "testExpectEqualFails (unittest_equality.EqualityTest) ... FAIL\r\n",
      "testExpectNotEqual (unittest_equality.EqualityTest) ... ok\r\n",
      "testExpectNotEqualFails (unittest_equality.EqualityTest) ... FAIL\r\n",
      "\r\n",
      "======================================================================\r\n",
      "FAIL: testExpectEqualFails (unittest_equality.EqualityTest)\r\n",
      "----------------------------------------------------------------------\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/Users/binyang/GitHub/Py3MOTW/Developer_Tools/unittest_equality.py\", line 11, in testExpectEqualFails\r\n",
      "    self.assertEqual(2, 3 - 2)\r\n",
      "AssertionError: 2 != 1\r\n",
      "\r\n",
      "======================================================================\r\n",
      "FAIL: testExpectNotEqualFails (unittest_equality.EqualityTest)\r\n",
      "----------------------------------------------------------------------\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/Users/binyang/GitHub/Py3MOTW/Developer_Tools/unittest_equality.py\", line 17, in testExpectNotEqualFails\r\n",
      "    self.assertNotEqual(1, 3 - 2)\r\n",
      "AssertionError: 1 == 1\r\n",
      "\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 4 tests in 0.001s\r\n",
      "\r\n",
      "FAILED (failures=2)\r\n"
     ]
    }
   ],
   "source": [
    "! python3 -m unittest -v unittest_equality.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Almost Equal?\n",
    "\n",
    "In addition to strict equality, it is possible to test for near equality of floating point numbers using assertAlmostEqual() and assertNotAlmostEqual()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# unittest_almostequal.py\n",
    "import unittest\n",
    "\n",
    "class AlmostEqualTest(unittest.TestCase):\n",
    "\n",
    "    def testEqual(self):\n",
    "        self.assertEqual(1.1, 3.3 - 2.2)\n",
    "\n",
    "    def testAlmostEqual(self):\n",
    "        self.assertAlmostEqual(1.1, 3.3 - 2.2, places=1)\n",
    "\n",
    "    def testNotAlmostEqual(self):\n",
    "        self.assertNotAlmostEqual(1.1, 3.3 - 2.0, places=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments are the values to be compared, and the number of decimal places to use for the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".F.\n",
      "======================================================================\n",
      "FAIL: testEqual (unittest_almostequal.AlmostEqualTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/binyang/GitHub/Py3MOTW/Developer_Tools/unittest_almostequal.py\", line 7, in testEqual\n",
      "    self.assertEqual(1.1, 3.3 - 2.2)\n",
      "AssertionError: 1.1 != 1.0999999999999996\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.002s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    }
   ],
   "source": [
    "! python3 -m unittest unittest_almostequal.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Containers\n",
    "\n",
    "In addition to the generic assertEqual() and assertNotEqual(), there are special methods for comparing containers like list, dict, and set objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# unittest_equality_container.py\n",
    "import textwrap\n",
    "import unittest\n",
    "\n",
    "\n",
    "class ContainerEqualityTest(unittest.TestCase):\n",
    "\n",
    "    def testCount(self):\n",
    "        self.assertCountEqual(\n",
    "            [1, 2, 3, 2],\n",
    "            [1, 3, 2, 3],\n",
    "        )\n",
    "\n",
    "    def testDict(self):\n",
    "        self.assertDictEqual(\n",
    "            {'a': 1, 'b': 2},\n",
    "            {'a': 1, 'b': 3},\n",
    "        )\n",
    "\n",
    "    def testList(self):\n",
    "        self.assertListEqual(\n",
    "            [1, 2, 3],\n",
    "            [1, 3, 2],\n",
    "        )\n",
    "\n",
    "    def testMultiLineString(self):\n",
    "        self.assertMultiLineEqual(\n",
    "            textwrap.dedent(\"\"\"\n",
    "            This string\n",
    "            has more than one\n",
    "            line.\n",
    "            \"\"\"),\n",
    "            textwrap.dedent(\"\"\"\n",
    "            This string has\n",
    "            more than two\n",
    "            lines.\n",
    "            \"\"\"),\n",
    "        )\n",
    "\n",
    "    def testSequence(self):\n",
    "        self.assertSequenceEqual(\n",
    "            [1, 2, 3],\n",
    "            [1, 3, 2],\n",
    "        )\n",
    "\n",
    "    def testSet(self):\n",
    "        self.assertSetEqual(\n",
    "            set([1, 2, 3]),\n",
    "            set([1, 3, 2, 4]),\n",
    "        )\n",
    "\n",
    "    def testTuple(self):\n",
    "        self.assertTupleEqual(\n",
    "            (1, 'a'),\n",
    "            (1, 'b'),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each method reports inequality using a format that is meaningful for the input type, making test failures easier to understand and correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFFFFFF\r\n",
      "======================================================================\r\n",
      "FAIL: testCount (unittest_equality_container.ContainerEqualityTest)\r\n",
      "----------------------------------------------------------------------\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/Users/binyang/GitHub/Py3MOTW/Developer_Tools/unittest_equality_container.py\", line 11, in testCount\r\n",
      "    [1, 3, 2, 3],\r\n",
      "AssertionError: Element counts were not equal:\r\n",
      "First has 2, Second has 1:  2\r\n",
      "First has 1, Second has 2:  3\r\n",
      "\r\n",
      "======================================================================\r\n",
      "FAIL: testDict (unittest_equality_container.ContainerEqualityTest)\r\n",
      "----------------------------------------------------------------------\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/Users/binyang/GitHub/Py3MOTW/Developer_Tools/unittest_equality_container.py\", line 17, in testDict\r\n",
      "    {'a': 1, 'b': 3},\r\n",
      "AssertionError: {'a': 1, 'b': 2} != {'a': 1, 'b': 3}\r\n",
      "- {'a': 1, 'b': 2}\r\n",
      "?               ^\r\n",
      "\r\n",
      "+ {'a': 1, 'b': 3}\r\n",
      "?               ^\r\n",
      "\r\n",
      "\r\n",
      "======================================================================\r\n",
      "FAIL: testList (unittest_equality_container.ContainerEqualityTest)\r\n",
      "----------------------------------------------------------------------\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/Users/binyang/GitHub/Py3MOTW/Developer_Tools/unittest_equality_container.py\", line 23, in testList\r\n",
      "    [1, 3, 2],\r\n",
      "AssertionError: Lists differ: [1, 2, 3] != [1, 3, 2]\r\n",
      "\r\n",
      "First differing element 1:\r\n",
      "2\r\n",
      "3\r\n",
      "\r\n",
      "- [1, 2, 3]\r\n",
      "+ [1, 3, 2]\r\n",
      "\r\n",
      "======================================================================\r\n",
      "FAIL: testMultiLineString (unittest_equality_container.ContainerEqualityTest)\r\n",
      "----------------------------------------------------------------------\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/Users/binyang/GitHub/Py3MOTW/Developer_Tools/unittest_equality_container.py\", line 37, in testMultiLineString\r\n",
      "    \"\"\"),\r\n",
      "AssertionError: '\\nThis string\\nhas more than one\\nline.\\n' != '\\nThis string has\\nmore than two\\nlines.\\n'\r\n",
      "  \r\n",
      "- This string\r\n",
      "+ This string has\r\n",
      "?            ++++\r\n",
      "- has more than one\r\n",
      "? ----           --\r\n",
      "+ more than two\r\n",
      "?           ++\r\n",
      "- line.\r\n",
      "+ lines.\r\n",
      "?     +\r\n",
      "\r\n",
      "\r\n",
      "======================================================================\r\n",
      "FAIL: testSequence (unittest_equality_container.ContainerEqualityTest)\r\n",
      "----------------------------------------------------------------------\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/Users/binyang/GitHub/Py3MOTW/Developer_Tools/unittest_equality_container.py\", line 43, in testSequence\r\n",
      "    [1, 3, 2],\r\n",
      "AssertionError: Sequences differ: [1, 2, 3] != [1, 3, 2]\r\n",
      "\r\n",
      "First differing element 1:\r\n",
      "2\r\n",
      "3\r\n",
      "\r\n",
      "- [1, 2, 3]\r\n",
      "+ [1, 3, 2]\r\n",
      "\r\n",
      "======================================================================\r\n",
      "FAIL: testSet (unittest_equality_container.ContainerEqualityTest)\r\n",
      "----------------------------------------------------------------------\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/Users/binyang/GitHub/Py3MOTW/Developer_Tools/unittest_equality_container.py\", line 49, in testSet\r\n",
      "    set([1, 3, 2, 4]),\r\n",
      "AssertionError: Items in the second set but not the first:\r\n",
      "4\r\n",
      "\r\n",
      "======================================================================\r\n",
      "FAIL: testTuple (unittest_equality_container.ContainerEqualityTest)\r\n",
      "----------------------------------------------------------------------\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/Users/binyang/GitHub/Py3MOTW/Developer_Tools/unittest_equality_container.py\", line 55, in testTuple\r\n",
      "    (1, 'b'),\r\n",
      "AssertionError: Tuples differ: (1, 'a') != (1, 'b')\r\n",
      "\r\n",
      "First differing element 1:\r\n",
      "a\r\n",
      "b\r\n",
      "\r\n",
      "- (1, 'a')\r\n",
      "?      ^\r\n",
      "\r\n",
      "+ (1, 'b')\r\n",
      "?      ^\r\n",
      "\r\n",
      "\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 7 tests in 0.005s\r\n",
      "\r\n",
      "FAILED (failures=7)\r\n"
     ]
    }
   ],
   "source": [
    "! python3 -m unittest unittest_equality_container.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use assertIn() to test container membership."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# unittest_in.py\n",
    "import unittest\n",
    "\n",
    "class ContainerMembershipTest(unittest.TestCase):\n",
    "\n",
    "    def testDict(self):\n",
    "        self.assertIn(4, {1: 'a', 2: 'b', 3: 'c'})\n",
    "\n",
    "    def testList(self):\n",
    "        self.assertIn(4, [1, 2, 3])\n",
    "\n",
    "    def testSet(self):\n",
    "        self.assertIn(4, set([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any object that supports the in operator or the container API can be used with assertIn()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFF\r\n",
      "======================================================================\r\n",
      "FAIL: testDict (unittest_in.ContainerMembershipTest)\r\n",
      "----------------------------------------------------------------------\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/Users/binyang/GitHub/Py3MOTW/Developer_Tools/unittest_in.py\", line 7, in testDict\r\n",
      "    self.assertIn(4, {1: 'a', 2: 'b', 3: 'c'})\r\n",
      "AssertionError: 4 not found in {1: 'a', 2: 'b', 3: 'c'}\r\n",
      "\r\n",
      "======================================================================\r\n",
      "FAIL: testList (unittest_in.ContainerMembershipTest)\r\n",
      "----------------------------------------------------------------------\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/Users/binyang/GitHub/Py3MOTW/Developer_Tools/unittest_in.py\", line 10, in testList\r\n",
      "    self.assertIn(4, [1, 2, 3])\r\n",
      "AssertionError: 4 not found in [1, 2, 3]\r\n",
      "\r\n",
      "======================================================================\r\n",
      "FAIL: testSet (unittest_in.ContainerMembershipTest)\r\n",
      "----------------------------------------------------------------------\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/Users/binyang/GitHub/Py3MOTW/Developer_Tools/unittest_in.py\", line 13, in testSet\r\n",
      "    self.assertIn(4, set([1, 2, 3]))\r\n",
      "AssertionError: 4 not found in {1, 2, 3}\r\n",
      "\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 3 tests in 0.001s\r\n",
      "\r\n",
      "FAILED (failures=3)\r\n"
     ]
    }
   ],
   "source": [
    "! python3 -m unittest unittest_in.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for Exceptions\n",
    "\n",
    "As previously mentioned, if a test raises an exception other than AssertionError it is treated as an error. This is very useful for uncovering mistakes while modifying code that has existing test coverage. There are circumstances, however, in which the test should verify that some code does produce an exception. For example, if an invalid value is given to an attribute of an object. In such cases, assertRaises() makes the code more clear than trapping the exception in the test. Compare these two tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# unittest_exception.py\n",
    "import unittest\n",
    "\n",
    "\n",
    "def raises_error(*args, **kwds):\n",
    "    raise ValueError('Invalid value: ' + str(args) + str(kwds))\n",
    "\n",
    "\n",
    "class ExceptionTest(unittest.TestCase):\n",
    "\n",
    "    def testTrapLocally(self):\n",
    "        try:\n",
    "            raises_error('a', b='c')\n",
    "        except ValueError:\n",
    "            pass\n",
    "        else:\n",
    "            self.fail('Did not see ValueError')\n",
    "\n",
    "    def testAssertRaises(self):\n",
    "        self.assertRaises(\n",
    "            ValueError,\n",
    "            raises_error,\n",
    "            'a',\n",
    "            b='c',\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results for both are the same, but the second test using assertRaises() is more succinct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testAssertRaises (unittest_exception.ExceptionTest) ... ok\r\n",
      "testTrapLocally (unittest_exception.ExceptionTest) ... ok\r\n",
      "\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 2 tests in 0.001s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "! python3 -m unittest -v unittest_exception.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Fixtures\n",
    "\n",
    "Fixtures are outside resources needed by a test. For example, tests for one class may all need an instance of another class that provides configuration settings or another shared resource. Other test fixtures include database connections and temporary files (many people would argue that using external resources makes such tests not “unit” tests, but they are still tests and still useful).\n",
    "\n",
    "unittest includes special hooks to configure and clean up any fixtures needed by tests. To establish fixtures for each individual test case, override setUp() on the TestCase. To clean them up, override tearDown(). To manage one set of fixtures for all instances of a test class, override the class methods setUpClass() and tearDownClass() for the TestCase. And to handle especially expensive setup operations for all of the tests within a module, use the module-level functions setUpModule() and tearDownModule()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# unittest_fixtures.py\n",
    "import random\n",
    "import unittest\n",
    "\n",
    "\n",
    "def setUpModule():\n",
    "    print('In setUpModule()')\n",
    "\n",
    "\n",
    "def tearDownModule():\n",
    "    print('In tearDownModule()')\n",
    "\n",
    "\n",
    "class FixturesTest(unittest.TestCase):\n",
    "\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        print('In setUpClass()')\n",
    "        cls.good_range = range(1, 10)\n",
    "\n",
    "    @classmethod\n",
    "    def tearDownClass(cls):\n",
    "        print('In tearDownClass()')\n",
    "        del cls.good_range\n",
    "\n",
    "    def setUp(self):\n",
    "        super().setUp()\n",
    "        print('\\nIn setUp()')\n",
    "        # Pick a number sure to be in the range. The range is\n",
    "        # defined as not including the \"stop\" value, so make\n",
    "        # sure it is not included in the set of allowed values\n",
    "        # for our choice.\n",
    "        self.value = random.randint(\n",
    "            self.good_range.start,\n",
    "            self.good_range.stop - 1,\n",
    "        )\n",
    "\n",
    "    def tearDown(self):\n",
    "        print('In tearDown()')\n",
    "        del self.value\n",
    "        super().tearDown()\n",
    "\n",
    "    def test1(self):\n",
    "        print('In test1()')\n",
    "        self.assertIn(self.value, self.good_range)\n",
    "\n",
    "    def test2(self):\n",
    "        print('In test2()')\n",
    "        self.assertIn(self.value, self.good_range)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When this sample test is run, the order of execution of the fixture and test methods is apparent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In setUpModule()\r\n",
      "In setUpClass()\r\n",
      "test1 (unittest_fixtures.FixturesTest) ... \r\n",
      "In setUp()\r\n",
      "In test1()\r\n",
      "In tearDown()\r\n",
      "ok\r\n",
      "test2 (unittest_fixtures.FixturesTest) ... \r\n",
      "In setUp()\r\n",
      "In test2()\r\n",
      "In tearDown()\r\n",
      "ok\r\n",
      "In tearDownClass()\r\n",
      "In tearDownModule()\r\n",
      "\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 2 tests in 0.000s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "! python3 -u -m unittest -v unittest_fixtures.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tearDown methods may not all be invoked if there are errors in the process of cleaning up fixtures. To ensure that a fixture is always released correctly, use addCleanup()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# unittest_addcleanup.py\n",
    "import random\n",
    "import shutil\n",
    "import tempfile\n",
    "import unittest\n",
    "\n",
    "def remove_tmpdir(dirname):\n",
    "    print('In remove_tmpdir()')\n",
    "    shutil.rmtree(dirname)\n",
    "\n",
    "class FixturesTest(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        super().setUp()\n",
    "        self.tmpdir = tempfile.mkdtemp()\n",
    "        self.addCleanup(remove_tmpdir, self.tmpdir)\n",
    "\n",
    "    def test1(self):\n",
    "        print('\\nIn test1()')\n",
    "\n",
    "    def test2(self):\n",
    "        print('\\nIn test2()')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example test creates a temporary directory and then uses shutil to clean it up when the test is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test1 (unittest_addcleanup.FixturesTest) ... \r\n",
      "In test1()\r\n",
      "In remove_tmpdir()\r\n",
      "ok\r\n",
      "test2 (unittest_addcleanup.FixturesTest) ... \r\n",
      "In test2()\r\n",
      "In remove_tmpdir()\r\n",
      "ok\r\n",
      "\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 2 tests in 0.005s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "! python3 -u -m unittest -v unittest_addcleanup.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeating Tests with Different Inputs\n",
    "\n",
    "It is frequently useful to run the same test logic with different inputs. Rather than defining a separate test method for each small case, a common way of doing this is to use one test method containing several related assertion calls. The problem with this approach is that as soon as one assertion fails, the rest are skipped. A better solution is to use subTest() to create a context for a test within a test method. If the test fails, the failure is reported and the remaining tests continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# unittest_subtest.py\n",
    "import unittest\n",
    "\n",
    "\n",
    "class SubTest(unittest.TestCase):\n",
    "\n",
    "    def test_combined(self):\n",
    "        self.assertRegex('abc', 'a')\n",
    "        self.assertRegex('abc', 'B')\n",
    "        # The next assertions are not verified!\n",
    "        self.assertRegex('abc', 'c')\n",
    "        self.assertRegex('abc', 'd')\n",
    "\n",
    "    def test_with_subtest(self):\n",
    "        for pat in ['a', 'B', 'c', 'd']:\n",
    "            with self.subTest(pattern=pat):\n",
    "                self.assertRegex('abc', pat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the test_combined() method never runs the assertions for the patterns 'c' and 'd'. The test_with_subtest() method does, and correctly reports the additional failure. Note that the test runner still considers there to only be two test cases, even though there are three failures reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_combined (unittest_subtest.SubTest) ... FAIL\r\n",
      "test_with_subtest (unittest_subtest.SubTest) ... \r\n",
      "======================================================================\r\n",
      "FAIL: test_combined (unittest_subtest.SubTest)\r\n",
      "----------------------------------------------------------------------\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/Users/binyang/GitHub/Py3MOTW/Developer_Tools/unittest_subtest.py\", line 9, in test_combined\r\n",
      "    self.assertRegex('abc', 'B')\r\n",
      "AssertionError: Regex didn't match: 'B' not found in 'abc'\r\n",
      "\r\n",
      "======================================================================\r\n",
      "FAIL: test_with_subtest (unittest_subtest.SubTest) (pattern='B')\r\n",
      "----------------------------------------------------------------------\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/Users/binyang/GitHub/Py3MOTW/Developer_Tools/unittest_subtest.py\", line 17, in test_with_subtest\r\n",
      "    self.assertRegex('abc', pat)\r\n",
      "AssertionError: Regex didn't match: 'B' not found in 'abc'\r\n",
      "\r\n",
      "======================================================================\r\n",
      "FAIL: test_with_subtest (unittest_subtest.SubTest) (pattern='d')\r\n",
      "----------------------------------------------------------------------\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/Users/binyang/GitHub/Py3MOTW/Developer_Tools/unittest_subtest.py\", line 17, in test_with_subtest\r\n",
      "    self.assertRegex('abc', pat)\r\n",
      "AssertionError: Regex didn't match: 'd' not found in 'abc'\r\n",
      "\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 2 tests in 0.002s\r\n",
      "\r\n",
      "FAILED (failures=3)\r\n"
     ]
    }
   ],
   "source": [
    "! python3 -m unittest -v unittest_subtest.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skipping Tests\n",
    "\n",
    "It is frequently useful to be able to skip a test if some external condition is not met. For example, when writing tests to check behavior of a library under a specific version of Python there is no reason to run those tests under other versions of Python. Test classes and methods can be decorated with skip() to always skip the tests. The decorators skipIf() and skipUnless() can be used to check a condition before skipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# unittest_skip.py\n",
    "import sys\n",
    "import unittest\n",
    "\n",
    "class SkippingTest(unittest.TestCase):\n",
    "\n",
    "    @unittest.skip('always skipped')\n",
    "    def test(self):\n",
    "        self.assertTrue(False)\n",
    "\n",
    "    @unittest.skipIf(sys.version_info[0] > 2,\n",
    "                     'only runs on python 2')\n",
    "    def test_python2_only(self):\n",
    "        self.assertTrue(False)\n",
    "\n",
    "    @unittest.skipUnless(sys.platform == 'Darwin',\n",
    "                         'only runs on macOS')\n",
    "    def test_macos_only(self):\n",
    "        self.assertTrue(True)\n",
    "\n",
    "    def test_raise_skiptest(self):\n",
    "        raise unittest.SkipTest('skipping via exception')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For complex conditions that are difficult to express in a single expression to be passed to skipIf() or skipUnless(), a test case may raise SkipTest directly to cause the test to be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test (unittest_skip.SkippingTest) ... skipped 'always skipped'\n",
      "test_macos_only (unittest_skip.SkippingTest) ... skipped 'only runs on macOS'\n",
      "test_python2_only (unittest_skip.SkippingTest) ... skipped 'only runs on python 2'\n",
      "test_raise_skiptest (unittest_skip.SkippingTest) ... skipped 'skipping via exception'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.000s\n",
      "\n",
      "OK (skipped=4)\n"
     ]
    }
   ],
   "source": [
    "! python3 -m unittest -v unittest_skip.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ignoring Failing Tests\n",
    "\n",
    "Rather than deleting tests that are persistently broken, they can be marked with the expectedFailure() decorator so the failure is ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# unittest_expectedfailure.py\n",
    "import unittest\n",
    "\n",
    "\n",
    "class Test(unittest.TestCase):\n",
    "\n",
    "    @unittest.expectedFailure\n",
    "    def test_never_passes(self):\n",
    "        self.assertTrue(False)\n",
    "\n",
    "    @unittest.expectedFailure\n",
    "    def test_always_passes(self):\n",
    "        self.assertTrue(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a test that is expected to fail does in fact pass, that condition is treated as a special sort of failure and reported as an “unexpected success”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_always_passes (unittest_expectedfailure.Test) ... unexpected success\r\n",
      "test_never_passes (unittest_expectedfailure.Test) ... expected failure\r\n",
      "\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 2 tests in 0.001s\r\n",
      "\r\n",
      "FAILED (expected failures=1, unexpected successes=1)\r\n"
     ]
    }
   ],
   "source": [
    "! python3 -m unittest -v unittest_expectedfailure.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
